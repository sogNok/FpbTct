{"cells":[{"cell_type":"markdown","metadata":{"id":"ozzz77CRi7Wj"},"source":["# Lab 06-1: Optimizers for Deep Neural Networks\n","## Exercise: Predicting MNIST Digits\n","### For this exercise, prepare Lab 05-3 to copy your previous implementations."]},{"cell_type":"markdown","metadata":{"id":"NhC9a_u5Ta4u"},"source":["### Prepare Mini-MNIST Dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"MboBxtwcTa41","executionInfo":{"status":"ok","timestamp":1656856958190,"user_tz":-540,"elapsed":259,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","\n","digits = load_digits()\n","\n","# digits.data from sklearn contains 1797 images of 8x8 pixels\n","# Each image has a hand-written digit\n","digits_df = digits.images.reshape((len(digits.target), -1))\n","digits_tf = digits.target\n","\n","# Splitting dataframe into train & test\n","X_train_org, X_test_org, y_train_num, y_test = train_test_split(digits_df, digits_tf, test_size= 0.20, random_state= 101)\n","\n","# Digits data has range of [0,16], which often lead too big exponential values\n","# so make them normal distribution of [0,1] with the sklearn package, or you can just divide them by 16\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train_org)\n","X_test = sc.transform(X_test_org)\n","\n","n_classes = 10\n","\n","# Transform Nx1 Y vector into Nx10 answer vector, so that we can perform one-to-all classification\n","y_train = np.zeros((y_train_num.shape[0],10))\n","for i in range(n_classes):\n","    y_train[:,i] = (y_train_num == i)"]},{"cell_type":"markdown","metadata":{"id":"CJECUexii7Wl"},"source":["Define Utility Functions"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"1O4m-iqzi7Wm","executionInfo":{"status":"ok","timestamp":1656856958191,"user_tz":-540,"elapsed":6,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def sigmoid(x):\n","    # Numerically stable with large exponentials\n","    x = np.where(x < 0, np.exp(x)/(1 + np.exp(x)), 1/(1 + np.exp(-x)))\n","    return x\n","\n","def softmax(x):\n","    # Numerically stable with large exponentials\n","    x = x - np.max(x, axis=-1, keepdims=True)\n","    x = np.exp(x)\n","    xs = np.sum(x, axis=-1, keepdims=True)\n","    return x / xs"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":436},"executionInfo":{"elapsed":372,"status":"ok","timestamp":1656856958558,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"zSR_GY3GTa44","outputId":"bb1cc0f7-76ab-4fd3-8acc-71a2831011c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1797, 64)\n","(1437, 64)\n","(1437, 10)\n","[ 0.  0.  0.  9. 16.  6.  0.  0.  0.  0.  4. 15.  6. 15.  0.  0.  0.  0.\n","  8. 11.  9. 11.  0.  0.  0.  0.  8. 16. 14.  2.  0.  0.  0.  0. 11. 16.\n"," 13.  0.  0.  0.  0.  6. 14.  2. 12.  9.  0.  0.  0.  5. 16. 11.  5. 13.\n","  4.  0.  0.  0.  3.  8. 13. 16.  9.  0.]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 288x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["The number is 5\n"]}],"source":["print(digits_df.shape)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_train_org[0])\n","\n","idx = np.random.randint(X_train.shape[0])\n","dimage = X_train_org[idx].reshape((8,8))\n","plt.gray()\n","plt.matshow(dimage)\n","plt.show()\n","print('The number is', y_train_num[idx])\n"]},{"cell_type":"markdown","metadata":{"id":"0WyDQogki7Wn"},"source":["### Simple DNN for Digit Classification"]},{"cell_type":"markdown","metadata":{"id":"GeAERpdhi7Wn"},"source":["Define Model Class (<b>From the previous exercise 05-3</b>)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Ba_C179li7Wn","executionInfo":{"status":"ok","timestamp":1656856958558,"user_tz":-540,"elapsed":12,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["class myNeuralLayer:\n","    def __init__(self, n_out, n_in):\n","        self.wegt = np.zeros((n_out, n_in))\n","        self.bias = np.zeros((n_out))\n","\n","    def forward(self, x):       # (b, i)\n","        ### START CODE HERE ###\n","\n","        x_lin = x @ self.wegt.T + self.bias            # Linear Prediction\n","        \n","        ### END CODE HERE ###\n","        return x_lin\n","\n","    def backward(self, x, x_in):  # x = dJ/dz (b, c)\n","        ### START CODE HERE ###\n","        \n","        dw = x.T @ x_in / x.shape[0]               # Gradients for weights\n","        db = np.mean(x, axis=0)               # Gradients for biases\n","        wdJdz = x @ self.wegt            # Propagation for lower layer\n","        \n","        ### END CODE HERE ###\n","        return dw, db, wdJdz\n"]},{"cell_type":"markdown","metadata":{"id":"_7sn3YU8i7Wo"},"source":["Define Backpropagation of Activation Functions (<b>From the previous exercise 05-3</b>)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HqgSJqzxi7Wo","executionInfo":{"status":"ok","timestamp":1656856958559,"user_tz":-540,"elapsed":12,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def dJdz_sigmoid(wdJdz_upper, az):\n","    ### START CODE HERE ###\n","\n","    dJdz = wdJdz_upper * az * (1 - az)            # backpropagation through activation function\n","\n","    ### END CODE HERE ###\n","    return dJdz\n","\n","def dJdz_softmax(y_hat, y):\n","    ### START CODE HERE ###\n","\n","    dJdz = y_hat - y            # backpropagation through activation function\n","    \n","    ### END CODE HERE ###\n","    return dJdz"]},{"cell_type":"markdown","metadata":{"id":"niXWwsibi7Wo"},"source":["Define Training Functions (<b>From the previous exercise 05-3</b>)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"DwuUyHwhi7Wo","executionInfo":{"status":"ok","timestamp":1656856958559,"user_tz":-540,"elapsed":12,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def my_forward(l1, l2, l3, X_in):\n","    ### START CODE HERE ###\n","\n","    a_1 = sigmoid(l1.forward(X_in))                    # first stage forward\n","    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","\n","    ### END CODE HERE ###\n","    return a_1, a_2, a_3\n","\n","def my_backward(l1, l2, l3, a_1, a_2, a_3, X_in, y_true):\n","    ### START CODE HERE ###\n","\n","    dw_3, db_3, wdJdz_3 = l3.backward(dJdz_softmax(a_3, y_true), a_2)    # go through 3rd stage backward\n","    dw_2, db_2, wdJdz_2 = l2.backward(dJdz_sigmoid(wdJdz_3, a_2), a_1)    # go through 2nd stage backward\n","    dw_1, db_1, _       = l1.backward(dJdz_sigmoid(wdJdz_2, a_1), X_in)    # go through 1st stage backward\n","\n","    ### END CODE HERE ###\n","\n","    d_1 = [dw_1, db_1]\n","    d_2 = [dw_2, db_2]\n","    d_3 = [dw_3, db_3]\n","    \n","    return d_1, d_2, d_3\n","\n","def my_loss(l1, l2, l3, X_in, y_true):\n","    ### START CODE HERE ###\n","\n","    a_1 = sigmoid(l1.forward(X_in))                    # first stage forward\n","    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","    loss = -np.mean(y_true * l3.forward(a_2) - np.log(1 + np.exp(l3.forward(a_2))))\n","                                                     # calculate loss\n","\n","    ### END CODE HERE ###\n","    return loss\n","    \n","def my_predict(l1, l2, l3, X_in):\n","    ### START CODE HERE ###\n","\n","    a_1 = sigmoid(l1.forward(X_in))                    # first stage forward\n","    a_2 = sigmoid(l2.forward(a_1))                    # second stage forward\n","    a_3 = softmax(l3.forward(a_2))                   # third stage forward\n","    pred = np.argmax(a_3, axis=1)                   # make prediction\n","\n","    ### END CODE HERE ###\n","    return pred"]},{"cell_type":"markdown","metadata":{"id":"_Z21CRd9i7Wp"},"source":["Create a NN model and check the matrix dimensions"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"DT0rMw-rTa5A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656856958560,"user_tz":-540,"elapsed":12,"user":{"displayName":"이충섭","userId":"17624345601773915567"}},"outputId":"79a532bc-698a-4a41-d4ae-4706456dc890"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1437, 64) (1437, 10)\n","(80, 64) (80,)\n","(70, 80) (70,)\n","(10, 70) (10,)\n"]}],"source":["n_inputs  = 64\n","n_hidden1 = 80\n","n_hidden2 = 70\n","n_classes = 10\n","\n","l1 = myNeuralLayer(n_hidden1, n_inputs)\n","l2 = myNeuralLayer(n_hidden2, n_hidden1)\n","l3 = myNeuralLayer(n_classes, n_hidden2)\n","\n","print(X_train.shape, y_train.shape)\n","print(l1.wegt.shape, l1.bias.shape)\n","print(l2.wegt.shape, l2.bias.shape)\n","print(l3.wegt.shape, l3.bias.shape)"]},{"cell_type":"markdown","metadata":{"id":"3rl_xrrri7Wp"},"source":["Weight Initialization"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NdBr_hJUi7Wp","executionInfo":{"status":"ok","timestamp":1656856958560,"user_tz":-540,"elapsed":10,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["# Weights are initialized to...\n","l1.wegt = np.random.randn(n_hidden1, n_inputs)\n","l2.wegt = np.random.randn(n_hidden2, n_hidden1)\n","l3.wegt = np.random.randn(n_classes, n_hidden2)"]},{"cell_type":"markdown","metadata":{"id":"aXCZlANPTa46"},"source":["Define a Function for Splitting Dataset into mini-Batches"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"1_dZER6VTa49","executionInfo":{"status":"ok","timestamp":1656856958560,"user_tz":-540,"elapsed":9,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def create_mini_batches(X, y, batch_size=64):\n","    mini_batches = []\n","    n_minibatches = (X.shape[0] // batch_size)\n","    n_variables = X.shape[1]\n","    ### START CODE HERE ###\n","\n","    data = np.hstack((X,y))                  # concatenate X and y with np.hstack\n","    np.random.shuffle(data)      # then shuffle it\n","    \n","    for i in range(n_minibatches):\n","        mini_batch = data[i*batch_size:(i+1)*batch_size]        # get a slice of mini-batch\n","        X_mini, y_mini = mini_batch[:,:n_variables], mini_batch[:,n_variables:]    # split mini-batch into X & y\n","        mini_batches.append((X_mini, y_mini))\n","    \n","    if data.shape[0] % batch_size != 0:\n","        mini_batch = data[n_minibatches*batch_size:]        # process the remaining data\n","        X_mini, y_mini = mini_batch[:,:n_variables], mini_batch[:,n_variables:]    # split mini-batch into X & y\n","        mini_batches.append((X_mini, y_mini))\n","\n","    ### END CODE HERE ###\n","    return mini_batches"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t0qD-kW1i7Wq","executionInfo":{"status":"ok","timestamp":1656856958560,"user_tz":-540,"elapsed":9,"user":{"displayName":"이충섭","userId":"17624345601773915567"}},"outputId":"1fd984ef-cd8c-437a-e40d-cdc799b849af"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 4  5]\n"," [18 19]\n"," [12 13]\n"," [ 8  9]]\n","[[-12]\n"," [-19]\n"," [-16]\n"," [-14]] \n","\n","[[ 0  1]\n"," [ 6  7]\n"," [ 2  3]\n"," [14 15]]\n","[[-10]\n"," [-13]\n"," [-11]\n"," [-17]] \n","\n","[[16 17]\n"," [10 11]]\n","[[-18]\n"," [-15]] \n","\n"]}],"source":["np.random.seed(1)\n","\n","a = np.arange(20).reshape(10,2)\n","b = -np.arange(10,20).reshape(10,1)\n","\n","c = create_mini_batches(a, b, 4)\n","for mini_X, mini_y in c:\n","    print(mini_X)\n","    print(mini_y, '\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"VU_mpELmi7Wr"},"source":["expected outpu:\n","```\n","[[ 4  5]          [[ 0  1]           [[16 17] \n"," [18 19]           [ 6  7]            [10 11]] \n"," [12 13]           [ 2  3]           [[-18] \n"," [ 8  9]]          [14 15]]           [-15]]  \n","[[-12]            [[-10]             \n"," [-19]             [-13]             \n"," [-16]             [-11]             \n"," [-14]]            [-17]]              \n","```"]},{"cell_type":"markdown","metadata":{"id":"9x0jOGjMi7Wr"},"source":["## Define Various Optimizers\n","\n","Stochastic Gradient $$ g_t = \\nabla J(W_t,x^{(i)},y^{(i)}), \\;\\text{for mini-batch}\\; (i) \\to (i:i+n) $$\n","\n","SGD with momentum $$ \\Delta W(t) = \\gamma \\Delta W (t-1) + \\alpha \\cdot g_t $$\n","AdaGrad $$ \\Delta W(t) = {\\eta {1 \\over \\sqrt{\\delta_t + \\epsilon}}} \\odot g_t, \\;\\text{where}\\; \\delta_t = \\delta_{t-1} + g_t^2 $$\n","RMSProp $$ \\Delta W(t) = {\\eta {1 \\over \\sqrt{\\delta_t + \\epsilon}}} \\odot g_t, \\;\\text{where}\\; \\delta_t = \\beta \\delta_{t-1} + (1-\\beta) g_t^2 $$\n","Adam $$ \\Delta W(t) = {\\eta {\\hat{m}_t \\over \\sqrt{\\hat{v}_t} + \\epsilon}} \\odot g_t, \\;\\text{where}\\; \\hat{m}_t = {m_t \\over {1 - \\beta_1^t}}, \\; \\hat{v}_t = {v_t \\over {1 - \\beta_2^t}}, $$\n","$$ \\text{and}\\; m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t, \\; v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $$\n","\n","\n","In this experiment mini-batch gradient is used for all optimization methods unless mentioned otherwise.<br>\n","Investigate and discuss the effect on convergence of each optimizer"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ysjKcYzWi7Wr","executionInfo":{"status":"ok","timestamp":1656856958836,"user_tz":-540,"elapsed":282,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["class myOptParam:\n","    def __init__(self, n_out, n_in):\n","        # Previoud delta values for momentum optimizer\n","        self.W_dt = np.zeros((n_out, n_in))\n","        self.B_dt = np.zeros(n_out)\n","        # Variables for other optimizers\n","        self.W_mt = np.zeros((n_out, n_in))\n","        self.B_mt = np.zeros(n_out)\n","        self.W_vt = np.zeros((n_out, n_in))\n","        self.B_vt = np.zeros(n_out)\n","\n","def my_optimizer(lyr, opt, W_grad, B_grad, solver='sgd', learning_rate=0.01, iter=1):\n","    epsilon = 1e-8  # arbitrary small number\n","    alpha = eta = learning_rate\n","    if iter==0:\n","        print('iteration should start from 1.')\n","\n","    # optimizer routines\n","    if  solver=='sgd':\n","        W_dlt = alpha * W_grad\n","        B_dlt = alpha * B_grad\n","    elif solver=='momentum':\n","        gamma = 0.9               # default setting\n","        ### START CODE HERE ###\n","        \n","        W_dlt = gamma * opt.W_dt + alpha * W_grad              # momentum for previous delta\n","        B_dlt = gamma * opt.B_dt + alpha * B_grad              # same goes for bias\n","        opt.W_dt = W_dlt           # keep data for later use\n","        opt.B_dt = B_dlt           # for bias, too\n","\n","        ### END CODE HERE ###\n","    elif solver=='adagrad':\n","        ### START CODE HERE ###\n","\n","        opt.W_vt = opt.W_vt + W_grad**2           # accumulate delta square (2nd momentum)\n","        opt.B_vt = opt.B_vt + B_grad**2           # accumulater for bias term\n","        W_dlt = eta / np.sqrt(epsilon + opt.W_vt) * W_grad              # calculate new delta for weight\n","        B_dlt = eta / np.sqrt(epsilon + opt.B_vt) * B_grad              # and for bias\n","\n","        ### END CODE HERE ###\n","    elif solver=='rmsprop':\n","        beta2 = 0.9               # default setting\n","        ### START CODE HERE ###\n","\n","        opt.W_vt = beta2 * opt.W_vt + (1 - beta2) * W_grad**2           # blending with second momentum\n","        opt.B_vt = beta2 * opt.B_vt + (1 - beta2) * B_grad**2           # also doging samething for bias\n","        W_dlt = eta / np.sqrt(epsilon + opt.W_vt) * W_grad              # calculate new delta for weight\n","        B_dlt = eta / np.sqrt(epsilon + opt.B_vt) * B_grad              # and for bias\n","        \n","        ### END CODE HERE ###\n","    elif solver=='adam':\n","        beta1, beta2 = 0.9, 0.99  # default setting\n","        ### START CODE HERE ###\n","\n","        opt.W_mt = beta1 * opt.W_mt + (1 - beta1) * W_grad           # blending with first momentum\n","        opt.B_mt = beta1 * opt.B_mt + (1 - beta1) * B_grad           # first momentum for bias\n","        opt.W_vt = beta2 * opt.W_vt + (1 - beta2) * W_grad**2           # blending with second momentum\n","        opt.B_vt = beta2 * opt.B_vt + (1 - beta2) * B_grad**2           # second momentum for bias\n","        W_mc = opt.W_mt / (1 - beta1**iter)               # bias correction of first momentum for weight\n","        B_mc = opt.B_mt / (1 - beta1**iter)               # and for bias term\n","        W_vc = opt.W_vt / (1 - beta2**iter)               # bias correction of second momentum for weight\n","        B_vc = opt.B_vt / (1 - beta2**iter)               # and for bias term\n","        W_dlt = eta / (np.sqrt(W_vc) + epsilon) * W_mc              # calculate new delat for weight\n","        B_dlt = eta / (np.sqrt(B_vc) + epsilon) * B_mc              # and for bias\n","        \n","        ### END CODE HERE ###\n","    else:  \n","        print('optimizer error')\n","\n","    # Adjust weight\n","    lyr.wegt = lyr.wegt - W_dlt\n","    lyr.bias = lyr.bias - B_dlt\n","\n","    return"]},{"cell_type":"markdown","metadata":{"id":"zbxOpcpei7Wr"},"source":["Optimizer Test"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yWA1xBIUi7Wr","outputId":"24420592-3a98-4602-ed99-aaad3c1fda42","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656856958836,"user_tz":-540,"elapsed":2,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[ 3.12812265  6.11310547 -0.00659625] -5.620045550802262\n"]}],"source":["np.random.seed(1)\n","\n","lyr = myNeuralLayer(2,3)\n","opt = myOptParam(2,3)\n","\n","lyr.wegt = np.random.randn(2,3)\n","lyr.bias = np.random.randn(2)\n","opt.W_dt = np.random.randn(2,3)\n","opt.B_dt = np.random.randn(2)\n","opt.W_mt = np.random.randn(2,3)\n","opt.B_mt = np.random.randn(2)\n","opt.W_vt = np.abs(np.random.randn(2,3))\n","opt.B_vt = np.abs(np.random.randn(2))\n","\n","W_grad = np.random.randn(2,3)\n","B_grad = np.random.randn(2)\n","\n","# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n","my_optimizer(lyr, opt, W_grad, B_grad, 'adam', 10, 3)\n","print(lyr.wegt[0], lyr.bias[0])\n"]},{"cell_type":"markdown","metadata":{"id":"40n3sMCli7Wr"},"source":["**Expected Outputs**\n","\n","For SGD:\n","```\n","[8.49607236 7.8403     6.18428956] -14.853210006882223\n","```\n","For Momentum:\n","```\n","[8.20893718 8.06473334 4.86839241] -15.873602504984119\n","```\n","Fpr Adagrad:\n","```\n","[7.48870748 6.53638606 8.33582571] -7.1956640822563065\n","```\n","For RMSProp:\n","```\n","[ 9.04298939  9.58705745 16.48540342] -15.770618828227912\n","```\n","For Adam:\n","```\n","[ 3.12812265  6.11310547 -0.00659625] -5.620045550802262\n","```"]},{"cell_type":"markdown","metadata":{"id":"12YIDP1Oi7Wr"},"source":["Create Optimizer Parameters"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"2OHaj86Ei7Wr","executionInfo":{"status":"ok","timestamp":1656856958836,"user_tz":-540,"elapsed":1,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["o1 = myOptParam(n_hidden1, n_inputs)\n","o2 = myOptParam(n_hidden2, n_hidden1)\n","o3 = myOptParam(n_classes, n_hidden2)"]},{"cell_type":"markdown","metadata":{"id":"lcGLrUSHi7Wr"},"source":["Training Simple Neural Network Model (3 layer model)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68346,"status":"ok","timestamp":1656857027181,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"qODinrZlTa5C","outputId":"6442a66c-c8b1-41f0-fc24-6956c2ca44eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["12.74694836637254\n","Epoch:  100,  loss: 1.27469484\n","10.936657729221356\n","Epoch:  200,  loss: 1.09366577\n","10.97877375813984\n","Epoch:  300,  loss: 1.09787738\n","11.00203858759724\n","Epoch:  400,  loss: 1.10020386\n","11.019420327705722\n","Epoch:  500,  loss: 1.10194203\n","11.030209457107729\n","Epoch:  600,  loss: 1.10302095\n","11.040031988667803\n","Epoch:  700,  loss: 1.10400320\n","11.048016246015525\n","Epoch:  800,  loss: 1.10480162\n","11.054900672086465\n","Epoch:  900,  loss: 1.10549007\n","11.061748381969052\n","Epoch: 1000,  loss: 1.10617484\n"]}],"source":["# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n","# alpha is learning rate\n","optimizer ='adam'\n","alpha = 0.01\n","n_epochs = 1000\n","\n","for epoch in range(n_epochs):\n","\n","    batches = create_mini_batches(X_train, y_train, batch_size=64)\n","    for one_batch in batches:\n","        X_mini, y_mini = one_batch\n","        batch_len = X_mini.shape[0]  # last batch might have different length\n","\n","        # Forward Path\n","        a_1, a_2, a_3 = my_forward(l1, l2, l3, X_mini)\n","        \n","        # Backward Path\n","        d_1, d_2, d_3 = my_backward(l1, l2, l3, a_1, a_2, a_3, X_mini, y_mini)\n","\n","        dw_1, db_1 = d_1\n","        dw_2, db_2 = d_2\n","        dw_3, db_3 = d_3\n","        \n","        # Update weights and biases\n","        my_optimizer(l1, o1, dw_1, db_1, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n","        my_optimizer(l2, o2, dw_2, db_2, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n","        my_optimizer(l3, o3, dw_3, db_3, solver=optimizer, learning_rate=alpha, iter=epoch+1)\n","\n","    if ((epoch+1)%100==0):\n","        loss_J = my_loss(l1, l2, l3, X_train, y_train)\n","        print('Epoch: %4d,  loss: %10.8f' % (epoch+1, loss_J))"]},{"cell_type":"markdown","metadata":{"id":"U3WyXpA-i7Ws"},"source":["Evaluate Model Performance"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1656857027181,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"xMvLn6SJTa5D","outputId":"d5c89f37-19a6-4447-9ed6-ff87f453ac6d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9777777777777777"]},"metadata":{},"execution_count":15}],"source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = my_predict(l1, l2, l3, X_test)\n","\n","accuracy_score(y_pred, y_test)"]},{"cell_type":"markdown","metadata":{"id":"WieUxPz9Ta5F"},"source":["Neural Network from scikit-learn"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14458,"status":"ok","timestamp":1656857041636,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"w8AcitiaTa5G","outputId":"1b0c095b-351e-4017-e8f4-59a117aba971"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9694444444444444"]},"metadata":{},"execution_count":16}],"source":["from sklearn.neural_network import MLPClassifier\n","\n","mlp = MLPClassifier(hidden_layer_sizes=(80, 70, ), activation='logistic', solver='sgd', \\\n","                    alpha=0.01, learning_rate_init=0.01, max_iter=1000)\n","\n","# Training/Fitting the Model\n","mlp.fit(X_train, y_train_num)\n","\n","# Making Predictions\n","s_pred = mlp.predict(X_test)\n","accuracy_score(s_pred, y_test)"]},{"cell_type":"markdown","metadata":{"id":"BmZQrDH9n0PK"},"source":["### Test Model with a random sample\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1656857041636,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"0dtF_vG3Ta5H","outputId":"51bfdeaa-823e-4e46-ce7b-c4b7a1b4c46d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 288x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAL50lEQVR4nO3d34tc9R3G8edxjfgrJlCtiJE1lRIQoYlIqCiSKkqsYvaiFxEqRFrsRSsJLYj2JvoPyPaiCMEfFfyFRrMUaa0BE0RotUncrZrEYnSDCWoUiYkKCSafXsxJScPWPbue73dn9/N+wZCZ2dnzfHc3z5xzZs6cryNCAOa202Z6AADKo+hAAhQdSICiAwlQdCABig4k0BdFt73S9ru237N9b+GsR20fsP12yZyT8i6xvcX2Ttvv2F5bOO9M22/YHmvyHiiZ12QO2H7T9ouls5q8cdtv2R61va1w1kLbG23vtr3L9tUFs5Y0P9OJyyHb6zpZeETM6EXSgKQ9kn4g6QxJY5IuL5h3naQrJb1d6ee7SNKVzfX5kv5d+OezpHOb6/MkvS7px4V/xt9KekrSi5V+p+OSzq+U9bikXzbXz5C0sFLugKSPJQ12sbx+WKMvl/ReRLwfEUclPSNpVamwiHhV0uellj9B3kcRsaO5fljSLkkXF8yLiPiyuTmvuRQ7Ksr2Ikm3SHq4VMZMsb1AvRXDI5IUEUcj4mCl+Bsk7YmIvV0srB+KfrGkD0+6vU8FizCTbF8qaZl6a9mSOQO2RyUdkLQ5IkrmDUu6R9LxghmnCkkv295u+66COYslfSrpsWbX5GHb5xTMO9lqSU93tbB+KHoKts+V9LykdRFxqGRWRByLiKWSFklabvuKEjm2b5V0ICK2l1j+t7g2Iq6UdLOkX9u+rlDO6ert5j0UEcskfSWp6GtIkmT7DEm3SXquq2X2Q9H3S7rkpNuLmvvmDNvz1Cv5kxHxQq3cZjNzi6SVhSKukXSb7XH1drmut/1Eoaz/ioj9zb8HJG1Sb/evhH2S9p20RbRRveKXdrOkHRHxSVcL7Iei/1PSD20vbp7JVkv68wyPqTO2rd4+3q6IeLBC3gW2FzbXz5J0o6TdJbIi4r6IWBQRl6r3d3slIn5eIusE2+fYnn/iuqSbJBV5ByUiPpb0oe0lzV03SNpZIusUt6vDzXapt2kyoyLiG9u/kfQ39V5pfDQi3imVZ/tpSSsknW97n6T1EfFIqTz11np3SHqr2W+WpN9HxF8K5V0k6XHbA+o9kT8bEVXe9qrkQkmbes+fOl3SUxHxUsG8uyU92ayE3pd0Z8GsE09eN0r6VafLbV7KBzCH9cOmO4DCKDqQAEUHEqDoQAIUHUigr4pe+HDGGcsij7yZzuurokuq+cus+ocjj7yZzOu3ogMooMgBM7bn9FE4l1122ZS/59ChQzrvvPOmlXfs2LEpf8/hw4c1f/78aeUdOXJkyt/z9ddf6+yzz55W3nTG+cUXX2jBggXTytuzZ8+Uv+f48eM67bTprRen8/f7LiLCp95H0adhZGSkat7Bg7U+At0zPj5eNW/FihVV84aGhqrm1f77TVR0Nt2BBCg6kABFBxKg6EACFB1IgKIDCVB0IAGKDiTQqug1p0wC0L1Ji96cZPCP6p2C9nJJt9u+vPTAAHSnzRq96pRJALrXpuhppkwC5qrOzuvefFC+9md2AbTQpuitpkyKiA2SNkhz/9NrwGzTZtN9Tk+ZBGQw6Rq99pRJALrXah+9mSes1FxhAArjyDggAYoOJEDRgQQoOpAARQcSoOhAAhQdSICiAwkwU8s0lPidfZuxsbGqeXN9Zpg1a9ZUzauNmVqApCg6kABFBxKg6EACFB1IgKIDCVB0IAGKDiRA0YEEKDqQQJspmR61fcD22zUGBKB7bdbof5K0svA4ABQ0adEj4lVJn1cYC4BC2EcHEmDuNSCBzorO3GtA/2LTHUigzdtrT0v6u6QltvfZ/kX5YQHoUptJFm+vMRAA5bDpDiRA0YEEKDqQAEUHEqDoQAIUHUiAogMJUHQggc6OdZ9Jq1atqpq3d+/eqnlDQ0NV82rPhYbyWKMDCVB0IAGKDiRA0YEEKDqQAEUHEqDoQAIUHUiAogMJUHQggTYnh7zE9hbbO22/Y3ttjYEB6E6bY92/kfS7iNhhe76k7bY3R8TOwmMD0JE2c699FBE7muuHJe2SdHHpgQHozpT20W1fKmmZpNdLDAZAGa0/pmr7XEnPS1oXEYcm+DpzrwF9qlXRbc9Tr+RPRsQLEz2GudeA/tXmVXdLekTSroh4sPyQAHStzT76NZLukHS97dHm8tPC4wLQoTZzr70myRXGAqAQjowDEqDoQAIUHUiAogMJUHQgAYoOJEDRgQQoOpDAnJh7bfHixVXzBgcHq+Z98MEHVfPGxsaq5o2MjFTNu//++6vm9QPW6EACFB1IgKIDCVB0IAGKDiRA0YEEKDqQAEUHEqDoQAIUHUigzVlgz7T9hu2xZu61B2oMDEB32hzrfkTS9RHxZXN+99ds/zUi/lF4bAA60uYssCHpy+bmvObCBA3ALNJqH932gO1RSQckbY4I5l4DZpFWRY+IYxGxVNIiScttX3HqY2zfZXub7W1dDxLAdzOlV90j4qCkLZJWTvC1DRFxVURc1dXgAHSjzavuF9he2Fw/S9KNknaXHhiA7rR51f0iSY/bHlDvieHZiHix7LAAdKnNq+7/krSswlgAFMKRcUACFB1IgKIDCVB0IAGKDiRA0YEEKDqQAEUHEpgTc68NDw9XzRsdHa2aV9uKFSuq5q1fv75q3sGDB6vm1f7/ORHW6EACFB1IgKIDCVB0IAGKDiRA0YEEKDqQAEUHEqDoQAIUHUigddGbSRzetM2JIYFZZipr9LWSdpUaCIBy2k7JtEjSLZIeLjscACW0XaMPS7pH0vGCYwFQSJuZWm6VdCAitk/yOOZeA/pUmzX6NZJusz0u6RlJ19t+4tQHMfca0L8mLXpE3BcRiyLiUkmrJb0SET8vPjIAneF9dCCBKZ1KKiK2StpaZCQAimGNDiRA0YEEKDqQAEUHEqDoQAIUHUiAogMJUHQggTkx91ptW7dunekhFFX751u4cOGczusHrNGBBCg6kABFBxKg6EACFB1IgKIDCVB0IAGKDiRA0YEEKDqQQKtDYJtTPR+WdEzSN5zSGZhdpnKs+08i4rNiIwFQDJvuQAJtix6SXra93fZdJQcEoHttN92vjYj9tr8vabPt3RHx6skPaJ4AeBIA+lCrNXpE7G/+PSBpk6TlEzyGudeAPtVmNtVzbM8/cV3STZLeLj0wAN1ps+l+oaRNtk88/qmIeKnoqAB0atKiR8T7kn5UYSwACuHtNSABig4kQNGBBCg6kABFBxKg6EACFB1IgKIDCTD32jQMDw9XzRsZGamat3Tp0qp5a9eurZo3NDRUNa8fsEYHEqDoQAIUHUiAogMJUHQgAYoOJEDRgQQoOpAARQcSoOhAAq2Kbnuh7Y22d9veZfvq0gMD0J22x7r/QdJLEfEz22dIOrvgmAB0bNKi214g6TpJayQpIo5KOlp2WAC61GbTfbGkTyU9ZvtN2w83Ezn8D9t32d5me1vnowTwnbQp+umSrpT0UEQsk/SVpHtPfRBTMgH9q03R90naFxGvN7c3qld8ALPEpEWPiI8lfWh7SXPXDZJ2Fh0VgE61fdX9bklPNq+4vy/pznJDAtC1VkWPiFFJ7HsDsxRHxgEJUHQgAYoOJEDRgQQoOpAARQcSoOhAAhQdSMAR0f1C7e4X2kfGx8er5g0ODlbN27t3b9W82nPZ1c6rLSJ86n2s0YEEKDqQAEUHEqDoQAIUHUiAogMJUHQgAYoOJEDRgQQmLbrtJbZHT7ocsr2uxuAAdGPSc8ZFxLuSlkqS7QFJ+yVtKjwuAB2a6qb7DZL2RETdg6EBfCdTLfpqSU+XGAiAcloXvTmn+22Snvs/X2fuNaBPtZ3AQZJulrQjIj6Z6IsRsUHSBmnuf0wVmG2msul+u9hsB2alVkVvpkm+UdILZYcDoIS2UzJ9Jel7hccCoBCOjAMSoOhAAhQdSICiAwlQdCABig4kQNGBBCg6kABFBxIoNffap5Km85n18yV91vFw+iGLPPJq5Q1GxAWn3lmk6NNle1tEXDXXssgjb6bz2HQHEqDoQAL9VvQNczSLPPJmNK+v9tEBlNFva3QABVB0IAGKDiRA0YEEKDqQwH8AQd2o5ORSSQEAAAAASUVORK5CYII=\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["My prediction is 5\n","sk prediction is 5\n","Actual number is 5\n"]}],"source":["idx = np.random.randint(X_test.shape[0])\n","dimage = X_test_org[idx].reshape((8,8))\n","plt.gray()\n","plt.matshow(dimage)\n","plt.show()\n","\n","X_input = np.expand_dims(X_test[idx], 0)\n","\n","y_pred = my_predict(l1, l2, l3, X_input)\n","\n","s_pred = mlp.predict(X_input)\n","\n","print('My prediction is ' + str(y_pred[0]))\n","print('sk prediction is ' + str(s_pred[0]))\n","print('Actual number is ' + str(y_test[idx]))\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"Sb77_LZMyNjt","executionInfo":{"status":"ok","timestamp":1656857041636,"user_tz":-540,"elapsed":6,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"execution_count":17,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ML_L06_01_OPT_dist.ipynb","provenance":[]},"interpreter":{"hash":"19c3f3f12223855b5e5811df3b51e2142b7f938327ffb9b9a66299337f7b60d0"},"kernelspec":{"display_name":"Python 3.9.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}