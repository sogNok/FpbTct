{"cells":[{"cell_type":"markdown","metadata":{"id":"TWyvqGlE3nvE"},"source":["# Lab 06-3: Batch Normalization for DNNs\n","## Exercise: Predicting MNIST Digits\n","### For this exercise, prepare Lab 05-3 to copy your previous implementations."]},{"cell_type":"markdown","metadata":{"id":"NhC9a_u5Ta4u"},"source":["### Prepare Mini MNIST Dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"MboBxtwcTa41","executionInfo":{"status":"ok","timestamp":1657025277841,"user_tz":-540,"elapsed":1036,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","\n","digits = load_digits()\n","\n","# digits.data from sklearn contains 1797 images of 8x8 pixels\n","# Each image has a hand-written digit\n","digits_df = digits.images.reshape((len(digits.target), -1))\n","digits_tf = digits.target\n","\n","# Splitting dataframe into train & test\n","X_train_org, X_test_org, y_train_num, y_test = train_test_split(digits_df, digits_tf, test_size= 0.20, random_state= 101)\n","\n","# Digits data has range of [0,16], which often lead too big exponential values\n","# so make them normal distribution of [0,1] with the sklearn package, or you can just divide them by 16\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train_org)\n","X_test = sc.transform(X_test_org)\n","\n","n_classes = 10\n","\n","# Transform Nx1 Y vector to Nx10 answer vector, so that we can perform one-to-all classification\n","y_train = np.zeros((y_train_num.shape[0],10))\n","for i in range(n_classes):\n","    y_train[:,i] = (y_train_num == i)"]},{"cell_type":"markdown","metadata":{"id":"ymPXrylz3nvI"},"source":["Define Utility Functions"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GmpOkdNT3nvJ","executionInfo":{"status":"ok","timestamp":1657025277842,"user_tz":-540,"elapsed":5,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def sigmoid(x):\n","    # Numerically stable with large exponentials\n","    x = np.where(x < 0, np.exp(x)/(1 + np.exp(x)), 1/(1 + np.exp(-x)))\n","    return x\n","\n","def softmax(x):\n","    # Numerically stable with large exponentials\n","    x = x - np.max(x, axis=-1, keepdims=True)\n","    x = np.exp(x)\n","    xs = np.sum(x, axis=-1, keepdims=True)\n","    return x / xs\n","\n","def create_mini_batches(X, y, batch_size=64):\n","    # Another implementatioin example of creating mini-batches\n","    data = np.hstack((X, y))\n","    np.random.shuffle(data)\n","    X_batches, y_batches = np.split(data, (X.shape[1],), axis=1)\n","    X_mini = np.split(X_batches, np.arange(batch_size,len(X),batch_size), axis=0)\n","    y_mini = np.split(y_batches, np.arange(batch_size,len(X),batch_size), axis=0)\n","    mini_batches = zip(X_mini, y_mini)\n","    return mini_batches"]},{"cell_type":"markdown","metadata":{"id":"b-PtejsB3nvJ"},"source":["Show Dataset Information"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":696,"status":"ok","timestamp":1657025278534,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"zSR_GY3GTa44","outputId":"aeb0e0f3-b79e-44c1-d82d-34a79124da88"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1797, 64)\n","(1437, 64)\n","(1437, 10)\n","[ 0.  0.  0.  9. 16.  6.  0.  0.  0.  0.  4. 15.  6. 15.  0.  0.  0.  0.\n","  8. 11.  9. 11.  0.  0.  0.  0.  8. 16. 14.  2.  0.  0.  0.  0. 11. 16.\n"," 13.  0.  0.  0.  0.  6. 14.  2. 12.  9.  0.  0.  0.  5. 16. 11.  5. 13.\n","  4.  0.  0.  0.  3.  8. 13. 16.  9.  0.]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 288x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["The number is 8\n"]}],"source":["print(digits_df.shape)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_train_org[0])\n","\n","idx = np.random.randint(X_train.shape[0])\n","dimage = X_train_org[idx].reshape((8,8))\n","plt.gray()\n","plt.matshow(dimage)\n","plt.show()\n","print('The number is', y_train_num[idx])\n"]},{"cell_type":"markdown","metadata":{"id":"VOgFZeuT3nvL"},"source":["### Simple DNN for Digit Classification"]},{"cell_type":"markdown","metadata":{"id":"BbWhYWKk3nvL"},"source":["Define Model Class with a Dropout Mask<br>\n","Forward and Backward are same as before (<b>Exercise 05-3</b>)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"O1XYLS4G3nvL","executionInfo":{"status":"ok","timestamp":1657025278534,"user_tz":-540,"elapsed":12,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["class myNeuralLayer:\n","    def __init__(self, n_out, n_in):\n","        self.wegt = np.zeros((n_out, n_in))\n","        self.bias = np.zeros((n_out))\n","\n","    def forward(self, x):       # (b, i)\n","        ### START CODE HERE ###\n","\n","        x_lin = np.matmul(x, self.wegt.T) + self.bias            # Linear Prediction\n","        \n","        ### END CODE HERE ###\n","        return x_lin\n","\n","    def backward(self, x, x_in):  # x = dJ/dz (b, c)\n","        ### START CODE HERE ###\n","        \n","        dw = np.matmul(x.T, x_in) / x.shape[0]               # Gradients for weights\n","        db = np.mean(x, axis=0)               # Gradients for biases\n","        wdJdz = np.matmul(x, self.wegt)            # Propagation for lower layer\n","        \n","        ### END CODE HERE ###\n","        return dw, db, wdJdz\n"]},{"cell_type":"markdown","metadata":{"id":"oNDQiEDq3nvM"},"source":["Define Backpropagation of Activation Functions (<b>From the previous exercise 05-3</b>)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DbMSjUaD3nvM","executionInfo":{"status":"ok","timestamp":1657025278535,"user_tz":-540,"elapsed":11,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def dJdz_sigmoid(wdJdz_upper, az):\n","    ### START CODE HERE ###\n","\n","    dJdz = wdJdz_upper * az * (1 - az)            # backpropagation through activation function\n","\n","    ### END CODE HERE ###\n","    return dJdz\n","\n","def dJdz_softmax(y_hat, y):\n","    ### START CODE HERE ###\n","\n","    dJdz = y_hat - y            # backpropagation through activation function\n","\n","    ### END CODE HERE ###\n","    return dJdz"]},{"cell_type":"markdown","metadata":{"id":"s8Ov1yBK3nvM"},"source":["### Define Batch Normalization Layer\n","\n","Forward Path\n","$$ \\mu_{k,B} = {1 \\over m} \\sum_{i=1}^{m} x_k^{(i)}, \\quad \n","\\sigma_{k,B}^2 = {1 \\over m} \\sum_{i=1}^{m} \\left( x_k^{(i)} - \\mu_{k,B} \\right)^2 $$\n","\n","$$ \\hat{x}_k^{(i)} = {{x_k^{(i)} - \\mu_{k,B}} \\over {\\sqrt{\\sigma_{k,B}^{2} + \\epsilon}}}, \\quad \n","y_k^{(i)} = \\gamma_k \\hat{x}_k^{(i)} + \\beta_k $$\n","\n","Backward Path\n","$$ {\\partial J \\over \\partial \\beta_k} = \n","\\sum_{i=1}^{m} {\\partial J \\over \\partial y_k^{(i)}}, \\quad \n","{\\partial J \\over \\partial \\gamma_k} = \n","\\sum_{i=1}^{m} \\left( {\\partial J \\over \\partial y_k^{(i)}} \\cdot \\hat{x}_k^{(i)} \\right) $$\n","$$ {\\partial J \\over \\partial x_k^{(i)}} = \n","{1 \\over \\sqrt{\\sigma_{k,B}^2 + \\epsilon}} \\cdot {\\partial J \\over \\partial \\hat{x}_k^{(i)}} +\n","{2 \\over m} \\left( x_k^{(i)} - \\mu_{k,B} \\right) {\\partial J \\over \\partial \\sigma_{k,B}^{2}} +\n","{1 \\over m} \\cdot {\\partial J \\over \\partial \\mu_{k,B}} $$"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"W7vgpzz73nvN","executionInfo":{"status":"ok","timestamp":1657025278535,"user_tz":-540,"elapsed":11,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["class myBatchNorm:\n","    def __init__(self, n_out, batch_size):\n","        # Parameters for Batch Normalization\n","        self.gamm = np.ones(n_out)                    # trainable variabla\n","        self.beta = np.zeros(n_out)                   # trainable variable\n","        self.mean = np.zeros(n_out)                   # running mean \n","        self.vari = np.zeros(n_out)                   # running variance\n","\n","        # Transfering data from forward to backward\n","        self.t_mean = np.zeros(n_out)                 # mean \n","        self.t_xdif = np.zeros((n_out, batch_size))   # diff = x - mu\n","        self.t_vari = np.zeros(n_out)                 # variance\n","        self.t_stdv = np.zeros(n_out)                 # stadard deviation\n","        self.t_invs = np.zeros(n_out)                 # inverse standard deviation\n","        self.t_xhat = np.zeros((n_out, batch_size))   # x_hat = diff / std_dev\n","    \n","    def forward(self, x):\n","        epsilon = 1e-8\n","        batch_len = x.shape[0]\n","\n","        # forward batch normalization\n","        ### START CODE HERE ###\n","\n","        self.t_mean = np.mean(x, axis=0)                            # mean, np.mean(hidz_1, axis=0)\n","        self.t_xdif = x - self.t_mean                            # diff = x - mu\n","        self.t_vari = np.var(x, axis=0)                            # variance, np.var(), axis=0\n","        self.t_stdv = np.sqrt(self.t_vari + epsilon)                            # stadard deviation\n","        self.t_invs = 1 / self.t_stdv                           # inverse standard deviation\n","        self.t_xhat = self.t_xdif / self.t_stdv                            # x_hat = diff / std_dev\n","        xout = self.gamm * self.t_xhat + self.beta                                   # output\n","\n","        ### END CODE HERE ###\n","        \n","        return xout\n","\n","    def backward(self, x):\n","        momentum = 0.9  # Constant for running mean / variance.\n","        batch_len = x.shape[0]\n","\n","        # backward batch normalization\n","        ### START CODE HERE ###\n","\n","        dJdb = np.sum(x, axis=0)                                   # dJ/dbeta\n","        dJdg = np.sum(x * self.t_xhat, axis=0)                                   # dJ/dgamma\n","        dJdh = self.gamm * x                                   # dJ/dx_hat\n","        dJdv = np.sum(dJdh * self.t_xdif, axis=0) * (-(self.t_invs**3)/2)                                   # dJ/dv\n","        dJdm = -self.t_invs * np.sum(dJdh, axis=0) + dJdv * (-2) * np.mean(self.t_xdif, axis=0)                                   # dJ/dm\n","        dJdx = self.t_invs * dJdh + 2 / batch_len * self.t_xdif * dJdv + dJdm / batch_len                                   # dJ/dx\n","\n","        # Parameter update\n","        self.beta = self.beta - 0.01 * dJdb                              # update beta\n","        self.gamm = self.gamm - 0.01 * dJdg                              # update gamma\n","        self.mean = momentum * self.t_mean + (1 - momentum) * self.t_mean                               # update mean\n","        self.vari = momentum * self.t_vari + (1 - momentum) * self.t_vari                              # update variance\n","        ### END CODE HERE ###\n","        \n","        return dJdx\n","\n","    def predict(self, x):\n","        epsilon = 1e-8\n","        # batch normalization layer for prediction\n","        ### START CODE HERE ###\n","\n","        x = (x - self.mean) / np.sqrt(self.vari + epsilon)                                      # apply mean and variance\n","        x = self.gamm * x + self.beta                                      # apply gamma and beta\n","\n","        ### END CODE HERE ###\n","        \n","        return x\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"BvX2ccRs3nvN","outputId":"45ac0d4d-c256-4260-9f83-1a0cee6586f9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657025278536,"user_tz":-540,"elapsed":10,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["forward:\n"," [[ 1.41726481 -0.96839351  0.21649074]\n"," [-1.08277462  0.93413175 -1.34749376]]\n","backward:\n"," [[ 0.98604152  1.06920658  0.27447564]\n"," [ 1.07963509 -0.40311355  0.04751336]]\n","predict:\n"," [[-0.765231    2.04261525  0.70530959]\n"," [-0.66411125  0.07723286  2.48300419]]\n"]}],"source":["np.random.seed(1)\n","bn = myBatchNorm(3,7)\n","a = bn.forward(np.random.randn(7,3))\n","b = bn.backward(np.random.randn(7,3))\n","c = bn.predict(np.random.randn(7,3))\n","print('forward:\\n', a[0:2])\n","print('backward:\\n', b[0:2])\n","print('predict:\\n', c[0:2])"]},{"cell_type":"markdown","metadata":{"id":"NfSg3iCY3nvN"},"source":["Expected Output:\n","```\n","forward:\n"," [[ 1.41726481 -0.96839351  0.21649074]\n"," [-1.08277462  0.93413175 -1.34749376]]\n","backward:\n"," [[ 0.98604152  1.06920658  0.27447564]\n"," [ 1.07963509 -0.40311355  0.04751336]]\n","predict:\n"," [[ 0.72592995 19.03321851 -1.21365873]\n"," [ 0.73705164  2.96814124 -4.91310599]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"u7IE5OMW3nvO"},"source":["Define Training Functions<br>\n","<b>Batch Normalization should be implemented (Based on the code from exercise 05-3</b>)"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"vgCakGbv3nvO","executionInfo":{"status":"ok","timestamp":1657025894360,"user_tz":-540,"elapsed":293,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def my_forward(l1, l2, l3, b1, b2, X_in):\n","    ### START CODE HERE ###\n","\n","    x_1 = l1.forward(X_in)                    # first linear layer\n","    h_1 = b1.forward(x_1)                    # first batchnorm layer\n","    a_1 = sigmoid(h_1)                    # first activation layer\n","    \n","    x_2 = l2.forward(a_1)                    # second linear layer\n","    h_2 = b2.forward(x_2)                    # second batchnorm layer\n","    a_2 = sigmoid(h_2)                    # second activation layer\n","\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","\n","    ### END CODE HERE ###\n","    return a_1, a_2, a_3\n","\n","def my_backward(l1, l2, l3, b1, b2, a_1, a_2, a_3, X_in, y_true):\n","    ### START CODE HERE ###\n","\n","    dw_3, db_3, wdJdz_3 = l3.backward(dJdz_softmax(a_3, y_true), a_2)    # go through 3rd stage backward\n","    dJdz_2 = dJdz_sigmoid(wdJdz_3, a_2)                 # second activation backward\n","    dJdx_2 = b2.backward(dJdz_2)                 # second batchnorm backward\n","    dw_2, db_2, wdJdz_2 = l2.backward(dJdx_2, a_1)    # backward through 2ns linear layer\n","    dJdz_1 = dJdz_sigmoid(wdJdz_2, a_1)                 # first activation backward\n","    dJdx_1 = b1.backward(dJdz_1)                 # first batchnorm backward\n","    dw_1, db_1, _       = l1.backward(dJdx_1, X_in)    # backward through 1st linear layer\n","\n","    ### END CODE HERE ###\n","    d_1 = [dw_1, db_1]\n","    d_2 = [dw_2, db_2]\n","    d_3 = [dw_3, db_3]\n","    return d_1, d_2, d_3\n","\n","def my_loss(l1, l2, l3, b1, b2, X_in, y_true):\n","    ### START CODE HERE ###\n","\n","    x_1 = l1.forward(X_in)                    # first linear layer\n","    h_1 = b1.forward(x_1)                    # first batchnorm layer\n","    a_1 = sigmoid(h_1)                    # first activation layer\n","    x_2 = l2.forward(a_1)                    # second linear layer\n","    h_2 = b2.forward(x_2)                    # second batchnorm layer\n","    a_2 = sigmoid(h_2)                    # second activation layer\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","    loss = -np.mean(y_true * np.log(a_3))                   # calculate loss\n","\n","    ### END CODE HERE ###\n","    return loss\n","    \n","def my_predict(l1, l2, l3, b1, b2, X_in):\n","    ### START CODE HERE ###\n","\n","    x_1 = l1.forward(X_in)                    # first linear layer\n","    h_1 = b1.forward(x_1)                    # first batchnorm layer\n","    a_1 = sigmoid(h_1)                    # first activation layer\n","    x_2 = l2.forward(a_1)                   # second linear layer\n","    h_2 = b2.forward(x_2)                    # second batchnorm layer\n","    a_2 = sigmoid(h_2)                    # second activation layer\n","    a_3 = softmax(l3.forward(a_2))                    # third stage forward\n","    pred = np.argmax(a_3, axis=1)                   # predict the class\n"," \n","    ### END CODE HERE ###\n","    return pred"]},{"cell_type":"markdown","metadata":{"id":"PKAgvPKS3nvO"},"source":["Create a NN model and check the matrix dimensions"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"DT0rMw-rTa5A","outputId":"1db8b6f4-e7ea-462c-9d0a-b84548abae10","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657025278821,"user_tz":-540,"elapsed":15,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(1437, 64) (1437, 10)\n","(80, 64) (80,)\n","(70, 80) (70,)\n","(10, 70) (10,)\n"]}],"source":["n_inputs  = 64\n","n_hidden1 = 80\n","n_hidden2 = 70\n","n_classes = 10\n","\n","l1 = myNeuralLayer(n_hidden1, n_inputs)\n","l2 = myNeuralLayer(n_hidden2, n_hidden1)\n","l3 = myNeuralLayer(n_classes, n_hidden2)\n","\n","print(X_train.shape, y_train.shape)\n","print(l1.wegt.shape, l1.bias.shape)\n","print(l2.wegt.shape, l2.bias.shape)\n","print(l3.wegt.shape, l3.bias.shape)"]},{"cell_type":"markdown","metadata":{"id":"tfNq4pfC3nvO"},"source":["Expected Output:\n","```\n","(1437, 64) (1437, 10)\n","(80, 64) (80,)\n","(70, 80) (70,)\n","(10, 70) (10,)\n","```"]},{"cell_type":"markdown","metadata":{"id":"0A0xDu1Y3nvP"},"source":["Weight Initialization"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ljiVbQ2Q3nvP","executionInfo":{"status":"ok","timestamp":1657025278821,"user_tz":-540,"elapsed":10,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["def my_initializer(lyr, pdf='normal'):   # probability density function\n","    (fan_out, fan_in) = w_shape = lyr.wegt.shape                # (c, i)\n","    if pdf=='normal':\n","        lyr.wegt = np.random.randn(*w_shape)\n","    elif pdf=='uniform':\n","        lyr.wegt = np.random.rand(*w_shape)\n","    elif pdf=='xavier_normal':           # Xavier is also known as Glorot\n","        lyr.wegt = np.random.randn(*w_shape) * np.sqrt(2/(fan_out + fan_in))\n","    elif pdf=='xavier_uniform':\n","        lyr.wegt = np.random.rand(*w_shape) * np.sqrt(6/(fan_out + fan_in))\n","    elif pdf=='he_normal':\n","        lyr.wegt = np.random.randn(*w_shape) * np.sqrt(2/fan_in)\n","    elif pdf=='he_uniform':\n","        lyr.wegt = np.random.rand(*w_shape) * np.sqrt(6/fan_in)\n","    else:\n","        print('initializer error')\n","    return\n","\n","# Weights are initialized to...\n","weight_init = 'he_normal'\n","\n","my_initializer(l1, pdf=weight_init)\n","my_initializer(l2, pdf=weight_init)\n","my_initializer(l3, pdf=weight_init)"]},{"cell_type":"markdown","metadata":{"id":"QF8kHNVZ3nvP"},"source":["Define Optimizer; Mini-batch SGD only."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ZRiyBfp13nvP","executionInfo":{"status":"ok","timestamp":1657025278821,"user_tz":-540,"elapsed":9,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["class myOptParam:\n","    def __init__(self, n_out, n_in):\n","        # Previoud delta values for momentum optimizer\n","        self.W_dt = np.zeros((n_out, n_in))\n","        self.B_dt = np.zeros(n_out)\n","        # Variables for other optimizers\n","        self.W_mt = np.zeros((n_out, n_in))\n","        self.B_mt = np.zeros(n_out)\n","        self.W_vt = np.zeros((n_out, n_in))\n","        self.B_vt = np.zeros(n_out)\n","        # Variable for Adam optimizer\n","        self.iter = 0\n","\n","def my_optimizer(lyr, opt, W_grad, B_grad, solver='sgd', learning_rate=0.01, iter=0):\n","    epsilon = 1e-8  # arbitrary small number\n","    alpha = eta = learning_rate\n","    if iter!=0:\n","        opt.iter = iter\n","\n","    # optimizer routines\n","    if  solver=='sgd':\n","        W_dlt = alpha * W_grad\n","        B_dlt = alpha * B_grad\n","    else:  \n","        print('optimizer error')\n","\n","    # Adjust weight\n","    lyr.wegt = lyr.wegt - W_dlt\n","    lyr.bias = lyr.bias - B_dlt\n","\n","    return"]},{"cell_type":"markdown","metadata":{"id":"PuWTM7ZN3nvP"},"source":["Create Optimizer Parameters"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"JDNMcJYQ3nvP","executionInfo":{"status":"ok","timestamp":1657025278821,"user_tz":-540,"elapsed":9,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["o1 = myOptParam(n_hidden1, n_inputs)\n","o2 = myOptParam(n_hidden2, n_hidden1)\n","o3 = myOptParam(n_classes, n_hidden2)"]},{"cell_type":"markdown","metadata":{"id":"orqHEly23nvP"},"source":["Training Simple Neural Network Model (3 layer model)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"b4iDDn8e3nvP","executionInfo":{"status":"ok","timestamp":1657025278822,"user_tz":-540,"elapsed":9,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["# optimizer settings are: 'sgd', 'momentum', 'adagrad', 'rmsprop', 'adam'\n","# alpha is learning rate\n","optimizer ='sgd'\n","alpha = 1e-2      # learning rate\n","lmbda = 1e-3      # l2 lambda\n","dpout = 0.5       # dropout rate\n","n_batch = 64      # batch size"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"pqUwY95e3nvP","executionInfo":{"status":"ok","timestamp":1657025278822,"user_tz":-540,"elapsed":8,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"outputs":[],"source":["b1 = myBatchNorm(n_hidden1, n_batch)\n","b2 = myBatchNorm(n_hidden2, n_batch)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94899,"status":"ok","timestamp":1657025373714,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"qODinrZlTa5C","outputId":"1c94dd8a-bb5d-481e-e3a5-8e87a50ac65e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:  100,  loss: 0.02012475\n","Epoch:  200,  loss: 0.00881271\n","Epoch:  300,  loss: 0.00504904\n","Epoch:  400,  loss: 0.00329515\n","Epoch:  500,  loss: 0.00234306\n","Epoch:  600,  loss: 0.00176021\n","Epoch:  700,  loss: 0.00138161\n","Epoch:  800,  loss: 0.00112004\n","Epoch:  900,  loss: 0.00092960\n","Epoch: 1000,  loss: 0.00078957\n"]}],"source":["n_epochs = 1000\n","\n","for epoch in range(n_epochs):\n","\n","    batches = create_mini_batches(X_train, y_train, batch_size=n_batch)\n","    for one_batch in batches:\n","        X_mini, y_mini = one_batch\n","        batch_len = X_mini.shape[0]  # last batch might have different length\n","\n","        # Forward Path\n","        a_1, a_2, a_3 = my_forward(l1, l2, l3, b1, b2, X_mini)\n","        \n","        # Backward Path\n","        d_1, d_2, d_3 = my_backward(l1, l2, l3, b1, b2, a_1, a_2, a_3, X_mini, y_mini)\n","\n","        dw_1, db_1 = d_1\n","        dw_2, db_2 = d_2\n","        dw_3, db_3 = d_3\n","        \n","        # Update weights and biases\n","        my_optimizer(l1, o1, dw_1, db_1, solver=optimizer, learning_rate=alpha)\n","        my_optimizer(l2, o2, dw_2, db_2, solver=optimizer, learning_rate=alpha)\n","        my_optimizer(l3, o3, dw_3, db_3, solver=optimizer, learning_rate=alpha)\n","\n","    if ((epoch+1)%100==0):\n","        loss_J = my_loss(l1, l2, l3, b1, b2, X_train, y_train)\n","        print('Epoch: %4d,  loss: %10.8f' % (epoch+1, loss_J))"]},{"cell_type":"markdown","metadata":{"id":"3Wbkfsdc3nvQ"},"source":["Evaluate Model Performance"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1657025373715,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"xMvLn6SJTa5D","outputId":"54466a4b-db60-4444-e6f8-f24cdfe3956f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9861111111111112"]},"metadata":{},"execution_count":16}],"source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = my_predict(l1, l2, l3, b1, b2, X_test)\n","\n","accuracy_score(y_pred, y_test)"]},{"cell_type":"markdown","metadata":{"id":"WieUxPz9Ta5F"},"source":["Neural Network from scikit-learn"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17808,"status":"ok","timestamp":1657025391515,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"w8AcitiaTa5G","outputId":"32766e09-951e-4c1a-c384-4121287e75db"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.975"]},"metadata":{},"execution_count":17}],"source":["from sklearn.neural_network import MLPClassifier\n","\n","mlp = MLPClassifier(hidden_layer_sizes=(80, 70, ), activation='logistic', solver='sgd', \\\n","                    alpha=0.01, learning_rate_init=0.01, max_iter=1000)\n","\n","# Training/Fitting the Model\n","mlp.fit(X_train, y_train_num)\n","\n","# Making Predictions\n","s_pred = mlp.predict(X_test)\n","accuracy_score(s_pred, y_test)"]},{"cell_type":"markdown","metadata":{"id":"BmZQrDH9n0PK"},"source":["### Test Model with a random sample\n"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":973},"executionInfo":{"elapsed":962,"status":"ok","timestamp":1657026167373,"user":{"displayName":"이충섭","userId":"17624345601773915567"},"user_tz":-540},"id":"0dtF_vG3Ta5H","outputId":"3feb11e9-11e0-459c-8a36-eff0cb2b5856"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 288x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["354\n","[ 0.         -0.33774812  0.79797893  0.97198553  0.97108554  1.27953221\n"," -0.11255111 -0.12996005 -0.06103492 -0.62723734  0.29302407  0.2541609\n","  0.77442859  1.30104426  1.39080176 -0.13788933 -0.04991522 -0.71668518\n"," -1.73861489 -1.20423153 -0.18012702  1.31032548  0.63230214 -0.1254162\n"," -0.02638899 -0.77557418 -1.46428092 -1.49744919  0.51812461  1.09795751\n"," -0.35853971 -0.0528332   0.         -0.66509509 -1.03874611  0.15025238\n","  0.95978599 -0.47895331 -0.82287384  0.         -0.05980442 -0.53502797\n","  0.03197336  1.36692002  0.07089916 -1.49041367 -0.81085566 -0.09209335\n"," -0.03682402  0.76328181  1.32279479  1.23835455  0.4920343  -0.32176458\n"," -0.77525535 -0.21622453 -0.02638899 -0.29648738  0.6843522   0.43064334\n","  0.84846559  1.55018141 -0.03204287 -0.19864951]\n","[[ 0.         -0.33774812  0.79797893  0.97198553  0.97108554  1.27953221\n","  -0.11255111 -0.12996005 -0.06103492 -0.62723734  0.29302407  0.2541609\n","   0.77442859  1.30104426  1.39080176 -0.13788933 -0.04991522 -0.71668518\n","  -1.73861489 -1.20423153 -0.18012702  1.31032548  0.63230214 -0.1254162\n","  -0.02638899 -0.77557418 -1.46428092 -1.49744919  0.51812461  1.09795751\n","  -0.35853971 -0.0528332   0.         -0.66509509 -1.03874611  0.15025238\n","   0.95978599 -0.47895331 -0.82287384  0.         -0.05980442 -0.53502797\n","   0.03197336  1.36692002  0.07089916 -1.49041367 -0.81085566 -0.09209335\n","  -0.03682402  0.76328181  1.32279479  1.23835455  0.4920343  -0.32176458\n","  -0.77525535 -0.21622453 -0.02638899 -0.29648738  0.6843522   0.43064334\n","   0.84846559  1.55018141 -0.03204287 -0.19864951]]\n","[[ 0.         -0.33774812  0.79797893  0.97198553  0.97108554  1.27953221\n","  -0.11255111 -0.12996005 -0.06103492 -0.62723734  0.29302407  0.2541609\n","   0.77442859  1.30104426  1.39080176 -0.13788933 -0.04991522 -0.71668518\n","  -1.73861489 -1.20423153 -0.18012702  1.31032548  0.63230214 -0.1254162\n","  -0.02638899 -0.77557418 -1.46428092 -1.49744919  0.51812461  1.09795751\n","  -0.35853971 -0.0528332   0.         -0.66509509 -1.03874611  0.15025238\n","   0.95978599 -0.47895331 -0.82287384  0.         -0.05980442 -0.53502797\n","   0.03197336  1.36692002  0.07089916 -1.49041367 -0.81085566 -0.09209335\n","  -0.03682402  0.76328181  1.32279479  1.23835455  0.4920343  -0.32176458\n","  -0.77525535 -0.21622453 -0.02638899 -0.29648738  0.6843522   0.43064334\n","   0.84846559  1.55018141 -0.03204287 -0.19864951]]\n","My prediction is 8\n","sk prediction is 2\n","Actual number is 2\n"]}],"source":["idx = np.random.randint(X_test.shape[0])\n","dimage = X_test_org[idx].reshape((8,8))\n","plt.gray()\n","plt.matshow(dimage)\n","plt.show()\n","\n","X_input = np.expand_dims(X_test[idx], 0)\n","\n","y_pred = my_predict(l1, l2, l3, b1, b2, X_input)\n","\n","s_pred = mlp.predict(X_input)\n","\n","print('My prediction is ' + str(y_pred[0]))\n","print('sk prediction is ' + str(s_pred[0]))\n","print('Actual number is ' + str(y_test[idx]))\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"ee2OvOhOY3Ey","executionInfo":{"status":"ok","timestamp":1657026163894,"user_tz":-540,"elapsed":286,"user":{"displayName":"이충섭","userId":"17624345601773915567"}}},"execution_count":45,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"ML_L06_03_BNRM_dist.ipynb","provenance":[]},"interpreter":{"hash":"ab0787582b148005e56ebe9aaff49c38c248360ce5fb0676a65b6185761e7c4b"},"kernelspec":{"display_name":"Python 3.9.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}